{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e07fce-b637-4a68-b80e-b8e8d9264eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42950d9a-79a5-4bcb-bf49-c88444d0e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (300, 300)\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a979a9-9456-4fa5-a8fd-0531caf5b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTATION = A.Compose([A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d8962-cb8d-4595-9db6-8c1656c3fc5d",
   "metadata": {},
   "source": [
    "load the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be553f2e-9119-482b-8639-012703a9f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_file):\n",
    "    \"\"\"\n",
    "    This function loads the labels and returns the labels\n",
    "\n",
    "    Args:\n",
    "    label_file = File path of the json file.\n",
    "\n",
    "    Returns:\n",
    "    It returns the loaded labels\n",
    "    \"\"\"\n",
    "    with open(file=label_file, mode='r') as f:\n",
    "        return json.load(fp=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7e5cf-e77b-45b2-96e8-66e0ea30fd3b",
   "metadata": {},
   "source": [
    "preprocess image pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8e1b6de-20ea-4d50-97b0-89a3555a879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_image(image_path):\n",
    "    \"\"\"\n",
    "    This function read the image and converts to RGB format\n",
    "\n",
    "    Args:\n",
    "    image_path: Training image file path.\n",
    "\n",
    "    Returns:\n",
    "    It returns coloe corrected image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7393326-8b7d-4c4d-84be-d7cb3f7b21c3",
   "metadata": {},
   "source": [
    "preprocess a sample pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5010e146-2c15-4595-b657-a5d5cc660af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old ignore this code\n",
    "def process_sample(sample, image_dir):\n",
    "    \"\"\"\n",
    "    This function returns processed image, bounding box coordinates and label associated with the image.\n",
    "\n",
    "    Args:\n",
    "    Sample: Sample image from the training dataset.\n",
    "    image_dir: Training image path.\n",
    "\n",
    "    Return:\n",
    "    It processed image, bounding box coordinates and label associated with the image.\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(image_dir, sample['name'])\n",
    "    image = preprocessing_image(image_path=image_path)\n",
    "    \n",
    "    #place holders for the bounding boxes dimensions and labels\n",
    "    b_boxes = []\n",
    "    labels = []\n",
    "\n",
    "    #extarct bounding box dimensions and labels from sample\n",
    "    for item in sample['labels']:\n",
    "        x1, y1, x2, y2 = item['box2d']['x1'], item['box2d']['y1'], item['box2d']['x2'], item['box2d']['y2']\n",
    "        b_boxes.append([x1, y1, x2, y2])\n",
    "        labels.append(item['category'])\n",
    "\n",
    "    #resize the image\n",
    "    augmented = AUGMENTATION(image=image, bboxes=b_boxes)\n",
    "\n",
    "    return augmented['image'], np.array(b_boxes), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd6e6-e40e-4be0-aac9-7117d4db7cec",
   "metadata": {},
   "source": [
    "image genarator pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ab9f362-9a1d-4643-9e60-fc0b1b696876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_dir, label_file, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    This function generate batches of images, bounding boxes & labels\n",
    "\n",
    "    Args:\n",
    "    image_dir: Image path\n",
    "    label_file: class labels\n",
    "    batch_size: Training image batch size\n",
    "\n",
    "    Return:\n",
    "    It yields the batches of images, bounding boxes & labels\n",
    "\n",
    "    \"\"\"\n",
    "    labels_data = load_labels(label_file)\n",
    "    total_images = len(labels_data)\n",
    "\n",
    "    while True:\n",
    "        for i in range(0, total_images, batch_size):\n",
    "            batch_images = labels_data[i:i + batch_size]\n",
    "            images, b_boxes, labels, masks = [], [], [], []\n",
    "\n",
    "            for sample in batch_images:\n",
    "                img, box, lbl, mask = process_sample(sample, image_dir)\n",
    "                images.append(img)\n",
    "                b_boxes.append(box)\n",
    "                labels.append(lbl)\n",
    "                masks.append(masks)\n",
    "             #Convert to TensorFlow Ragged Tensors to handle varying sizes\n",
    "            images = np.array(images)  # (batch, 300, 300, 3)\n",
    "            boxes = tf.ragged.constant(b_boxes)  # Variable-length bounding boxes\n",
    "            labels = tf.ragged.constant(labels)  # Variable-length labels\n",
    "            masks = np.array(masks).reshape(-1, 300, 300, 1)  # Ensure masks have uniform shape\n",
    "\n",
    "            yield images, boxes, labels, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bead3-ca75-4c92-9659-9b4382b55b49",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87806bc1-2d63-4ebd-a391-d3462bdc4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data and file paths\n",
    "IMAGE_DIR = \"D:/learning_desk/bosch_assignment_bdd_100k/data/bdd100k/images/train\"\n",
    "LABEL_FILE = 'D:/learning_desk/bosch_assignment_bdd_100k/data/bdd100k/labels/bdd100k_labels_images_train.json'\n",
    "train_data = data_generator(IMAGE_DIR, LABEL_FILE, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae522add-dbf1-4b33-ac7c-7fef1238c6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object data_generator at 0x000001E7C65CEAE0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8500f19-2747-4e66-820b-0073facb5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample, image_dir):\n",
    "    \"\"\"\n",
    "    This function returns processed image, bounding box coordinates and label associated with the image.\n",
    "\n",
    "    Args:\n",
    "    Sample: Sample image from the training dataset.\n",
    "    image_dir: Training image path.\n",
    "\n",
    "    Return:\n",
    "    It processed image, bounding box coordinates and label associated with the image.\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(image_dir, sample['name'])\n",
    "    image = preprocessing_image(image_path=image_path)\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    #place holders for the bounding boxes dimensions and labels\n",
    "    b_boxes = []\n",
    "    labels = []\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    #extarct bounding box dimensions and labels from sample\n",
    "    for item in sample['labels']:\n",
    "        category = item[\"category\"]\n",
    "        if \"box2d\" in item:\n",
    "            x1, y1, x2, y2 = item['box2d']['x1'], item['box2d']['y1'], item['box2d']['x2'], item['box2d']['y2']\n",
    "            b_boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(item['category'])\n",
    "\n",
    "        if \"poly2d\" in item:\n",
    "            for poly in item[\"poly2d\"]:\n",
    "                poly_vertices = np.array(poly[\"vertices\"], dtype=np.int32).reshape((-1, 1, 2))\n",
    "                cv2.fillPoly(mask, [poly_vertices], color=255)  # Fill the polygon area in mask\n",
    "\n",
    "    #resize the image\n",
    "    augmented = AUGMENTATION(image=image, bboxes=b_boxes)\n",
    "    mask_resized = cv2.resize(mask, IMG_SIZE)\n",
    "\n",
    "    return augmented['image'], np.array(b_boxes), np.array(labels), mask_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3777f9fc-4b88-48a4-b5a5-91ae4e74babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (300, 300, 3)\n",
      "Bounding Boxes: (7, 4)\n",
      "Labels: (7,)\n",
      "Mask shape: (300, 300)\n"
     ]
    }
   ],
   "source": [
    "labels_data = load_labels(LABEL_FILE)\n",
    "\n",
    "# Test one sample\n",
    "sample = labels_data[0]\n",
    "image, b_boxes, labels, mask = process_sample(sample, IMAGE_DIR)\n",
    "\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Bounding Boxes:\", b_boxes.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "print(\"Mask shape:\", mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd389319-e01e-4d38-9f14-99fd6729ea6d",
   "metadata": {},
   "source": [
    "model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ab64525-1873-48ed-9db1-050e14b8a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"This function builds an object detection model using MobileNetV2 as a feature extractor.\n",
    "    \"\"\"\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=(300, 300, 3), include_top=False)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"precision\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba4bc173-bd79-4d89-a07c-3eb6b5ac1203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vijiv\\AppData\\Local\\Temp\\ipykernel_9132\\1575517135.py:4: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = tf.keras.applications.MobileNetV2(input_shape=(300, 300, 3), include_top=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "368046d3-40e5-4b33-ab3d-077dccecb498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/learning_desk/bosch_assignment_bdd_100k/models/ssd_mobilenet_v2_bdd.h5'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = 'D:/learning_desk/bosch_assignment_bdd_100k/models/'\n",
    "model_name = 'ssd_mobilenet_v2_bdd.h5'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c93409-792c-4e13-9426-bcf7a4d10c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, epochs=1, steps_per_epoch=100)\n",
    "model.save(filepath=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c7539-8eb4-4a5b-b5be-435264b98035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bosch_env",
   "language": "python",
   "name": "bosch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
